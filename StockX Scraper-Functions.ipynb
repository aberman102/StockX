{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import urllib.request\n",
    "import random\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Headers= {\n",
    "    'authority': 'stockx.com', \n",
    "    'appos': 'web',\n",
    "    'x-requested-with': 'XMLHttpRequest', \n",
    "    'user-agent': None,\n",
    "    'appversion': '0.1', \n",
    "    'accept': '*/*',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-mode': 'cors',\n",
    "    'sec-fetch-dest': 'empty', \n",
    "    'referer': None,\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cookie': '__cfduid=df10c577f09b276a092eca24cff7bd85d1596467982; language_code=en; stockx_market_country=US; _ga=GA1.2.1903714774.1596467984; _gid=GA1.2.487839851.1596467984; _px_uAB=MTk5Mnx0cnVl; _dd_r=1; _dd=45924477-7f1e-4a74-8d7e-e669a8eff8c1; _pxvid=c293e788-d59c-11ea-9a86-0242ac120007; tracker_device=4c0b225d-895d-4386-a7b7-ddcda0ae4417; _ALGOLIA=anonymous-97a32dd0-2851-4208-b4d1-7e112f624b9c; is_gdpr=false; cookie_policy_accepted=true; stockx_ip_region=US; stockx_session=c561d92d-82eb-4553-9ca6-e057893f77f1; _gcl_au=1.1.672336051.1596467987; below_retail_type=; bid_ask_button_type=; brand_tiles_version=v1; browse_page_tile_size_update_web=true; bulk_shipping_enabled=false; default_apple_pay=false; intl_payments=true; multi_edit_option=beatLowestAskBy; product_page_affirm_callout_enabled_web=false; related_products_length=v2; riskified_recover_updated_verbiage=false; sell_ui_refresh=true; show_order_status_covid_faq=undefined; show_all_as_number=false; show_bid_education=v4; show_bid_education_times=1; show_how_it_works=true; show_watch_modal=false; pdp_refactor_web=undefined; IR_gbd=stockx.com; _pk_ses.421.1a3e=*; ajs_user_id=%225a1bd21c-a5a9-11ea-a119-0a0fc094c9a6%22; ajs_anonymous_id=%220f004205-ae19-4dee-b6ea-c38d6361bafe%22; _px_f394gi7Fvmc43dfg_user_id=YzUxNDU1NDAtZDU5Yy0xMWVhLTgxZWEtZmIyMDJhMWYwZGFk; rskxRunCookie=0; rCookie=80fuufrr8r8hhzb7cykakawkhb4l; covid_banner=true; QuantumMetricUserID=5341f249d57ef23e5f9c8cb0103f7f1b; QuantumMetricSessionID=57ed0d98704647101a51c477c547c8c7; _px3=7c7508d30f714b793071f40fbbfb09d46b6bb094c7ebf3dd9d91904a7b224aeb:E9/gSze5LDrxfCXBxhh8KiN9cjcUJsxQZiIOztgx4iJS/oZx+SzIFSf3f5zyabWdaArTVW/v0vOX3mVjMeSOaQ==:1000:PiIz+xjr6JHJ5qtYzGHQTiNddIoG7CIkPq9sERqCpHNE0XQUGIxRAA40u6iPGwJAnwHAeD/K47yFIQqVkTzsrfBGlAv+DmYWJVbSjmaQTZ4BncpVz+J0ybgoz8QVsUoV1ZNajUkVMATSOP7PBSdYNv2GsNp8VKEmD5WjUhNuKYI=; _gat=1; _px_7125205957_cs=eyJpZCI6ImM1MTNiOTAwLWQ1OWMtMTFlYS04MWVhLWZiMjAyYTFmMGRhZCIsInN0b3JhZ2UiOnsiZyI6dHJ1ZX0sImV4cGlyYXRpb24iOjE1OTY0NzAzODQ2OTV9; stockx_product_visits=4; lastRskxRun=1596468586086; _pk_id.421.1a3e=fc044ff3a55a9889.1596467988.1.1596468586.1596467988.; IR_9060=1596468561960%7C0%7C1596467988441%7C%7C; IR_PI=c50a1cc1-d59c-11ea-8cd5-42010a246e2a%7C1596554961960; stockx_homepage=sneakers'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent_list = [\n",
    "   #Chrome\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    #Firefox\n",
    "    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating scraper functions that will grab all the URL's and Stock Keeping Units's of StockX for a particular brand and Year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def URL_Grabber(Brand, Year, initial_url):\n",
    "    Brand= Brand.lower()\n",
    "    Year= str(Year)\n",
    "    Headers['referer']= initial_url\n",
    "    Headers['user-agent']= random.choice(user_agent_list)\n",
    "    response = requests.get(url= initial_url, headers=Headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    hrefs= []\n",
    "    for a_elm in soup.find_all(\"a\"):\n",
    "        hrefs.append((a_elm.attrs[\"href\"]))\n",
    "    hrefs= hrefs[33:(33+40)]\n",
    "    list_of_urls= []\n",
    "    for shoe in hrefs:\n",
    "        if 'https://stockx.com' in shoe:\n",
    "            pass\n",
    "        elif Brand+'-' in shoe:\n",
    "            list_of_urls.append('https://stockx.com'+shoe)\n",
    "        else:\n",
    "            pass\n",
    "    return list_of_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Full_Page_URL_Grabber(Brand, Year, page):\n",
    "    Brand= Brand.lower()\n",
    "    Year= str(Year)\n",
    "    urls=[]\n",
    "    style_dict= {'nike': ['foamposite', 'kd', 'kobe', 'lebron', 'air-force', 'air-max','basketball', 'sb', 'footwear'], \n",
    "                 'adidas':['yeezy','ultra-boost', 'nmd', 'basketball', 'footwear']}\n",
    "    \n",
    "    if Brand=='jordan':\n",
    "        initial_url= 'https://stockx.com/retro-jordans/release-date?size_types=men&years='+Year+'&page='+str(page)\n",
    "        urls.append(URL_Grabber(Brand, Year, initial_url))\n",
    "        time.sleep(random.randrange(0,3))\n",
    "    elif Brand== 'nike':\n",
    "        Styles= style_dict[Brand]\n",
    "        for Style in Styles:\n",
    "            initial_url='https://stockx.com/'+Brand+'/'+Style+'/release-date?size_types=men&years='+Year+'&page='+str(page)\n",
    "            urls.append(URL_Grabber(Brand, Year, initial_url))\n",
    "            time.sleep(random.randrange(0,3))\n",
    "    elif Brand== 'adidas':\n",
    "        Styles= style_dict[Brand]\n",
    "        for Style in Styles:\n",
    "            initial_url='https://stockx.com/'+Brand+'/'+Style+'/release-date?size_types=men&years='+Year+'&page='+str(page)\n",
    "            urls.append(URL_Grabber(Brand, Year, initial_url))\n",
    "            time.sleep(random.randrange(0,3))\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_URLS(Brand, Year):\n",
    "    urls= []\n",
    "    for num in range(1,25): #max number of pages shown by StockX is 25\n",
    "        if len(Full_Page_URL_Grabber(Brand,Year,num))>0:\n",
    "            urls.append(Full_Page_URL_Grabber(Brand,Year,num))\n",
    "            print('Done:'+Brand+ str(num))\n",
    "        else:\n",
    "            pass\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_SKU(URL):\n",
    "    response = requests.get(url= URL, headers=Headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    test=soup.find_all('script', type= 'application/ld+json')\n",
    "    test[3].get_text()\n",
    "    S = test[3].get_text()\n",
    "    x = ast.literal_eval(re.search('({.+})', S).group(0))\n",
    "    try:\n",
    "        sku= x['sku']\n",
    "        return sku\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def Past_Sales(URL):\n",
    "    sku= Get_SKU(URL)\n",
    "    if not sku:\n",
    "        df= pd.DataFrame({'Price': [0],'Sale_Date': [0], 'Size': [0]})\n",
    "        df['Shoe']= str(URL)\n",
    "        df['Shoe']= df['Shoe'].map(lambda x: x.replace('https://stockx.com/', '')).map(lambda x: x.replace('-', ' '))\n",
    "        return df\n",
    "    else:\n",
    "        pass\n",
    "    web_beginning= 'https://stockx.com/api/products/'\n",
    "    web_end= '/activity?state=480&currency=USD&limit=250&page=1&sort=createdAt&order=DESC&country=US'\n",
    "    url2= web_beginning+sku+web_end\n",
    "    Headers['user-agent']= random.choice(user_agent_list)\n",
    "    response = requests.get(url= url2, headers=Headers)\n",
    "    json1=json.loads(response.content).get('ProductActivity', None)\n",
    "    if json1:    \n",
    "        df = pd.DataFrame(json1)\n",
    "        columns_rename= {\"chainId\": \"Sale_ID\", \"amount\": \"Price\", 'createdAt': 'Sale_Date', \"shoeSize\": \"Size\", \"productId\": \"Product_ID\", 'skuUuid': \"SKU\", \"state\":'state', \"customerId\":'Customer_ID', \"localAmount\":'Local_Amount', \"localCurrency\": 'Currency'}\n",
    "        columns_drop= ['Sale_ID', 'Product_ID', 'state', 'Customer_ID', 'Local_Amount', 'Currency']\n",
    "        df.rename(columns=columns_rename, inplace= True)\n",
    "        df.drop(columns=columns_drop, inplace= True)\n",
    "        df['Shoe']= str(URL)\n",
    "        df['Shoe']= df['Shoe'].map(lambda x: x.replace('https://stockx.com/', '')).map(lambda x: x.replace('-', ' '))\n",
    "        df.drop(columns= 'SKU', inplace= True)\n",
    "        time.sleep(random.randrange(0,3))\n",
    "    else:\n",
    "        df= pd.DataFrame({'Price': [0],'Sale_Date': [0], 'Size': [0]})\n",
    "        df['Shoe']= str(URL)\n",
    "        df['Shoe']= df['Shoe'].map(lambda x: x.replace('https://stockx.com/', '')).map(lambda x: x.replace('-', ' '))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shoe_Names(DataFrame):\n",
    "    DataFrame['Shoe']= DataFrame['Shoe'].map(lambda x: x.replace(' ', '-'))\n",
    "    list_of_shoes= list(DataFrame['Shoe'].unique())\n",
    "    return list_of_shoes\n",
    "\n",
    "\n",
    "def Shoe_Data_Scraper(Shoe):\n",
    "    WebAddress_Start= 'https://stockx.com/api/products/'\n",
    "    WebAddress_End= '?includes=market,360&currency=USD&country=US'\n",
    "    Headers['user-agent']= random.choice(user_agent_list)\n",
    "    response = requests.get(url= WebAddress_Start+Shoe+WebAddress_End, headers=Headers)\n",
    "    json2=json.loads(response.content)\n",
    "    json2= json2['Product']\n",
    "    df2= pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in json2.items()]))\n",
    "    df2= pd.DataFrame.transpose(pd.DataFrame(df2.loc[0]))\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bring_the_brand_together(Brand, Year):\n",
    "    urls= all_URLS(Brand,Year)\n",
    "    df= Past_Sales(urls[0])\n",
    "    for url in urls:\n",
    "        if urls==urls[0]:\n",
    "            pass\n",
    "        else:\n",
    "            df1=Past_Sales(url)\n",
    "            df=pd.concat([df,df1])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_nike_urls= []\n",
    "for List in nike_urls:\n",
    "    for i in range(len(List)):\n",
    "        if List[i]== []:\n",
    "            pass\n",
    "        else:\n",
    "            complete_nike_urls.append(List[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "for index in range(len(complete_nike_urls)):\n",
    "    for url in complete_nike_urls[index]:\n",
    "        urls.append(url)\n",
    "urls= list(np.unique(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get around being stopped by StockX's API. Run this for loop and once you are stopped by StockX, you need to go to their site and do the Captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Past_Sales(urls[0])\n",
    "counter= len(urls)\n",
    "for index, url in enumerate(urls):\n",
    "    if url==urls[0]:\n",
    "        pass\n",
    "    else:\n",
    "        df1=Past_Sales(url)\n",
    "        df=pd.concat([df,df1])\n",
    "        counter-=1\n",
    "        print(url, 'Shoes Left:',counter, 'Shoes Done:', len(np.unique(list(df['Shoe'].values))))\n",
    "        if index%10==0:\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            pass\n",
    "        if index%100==0:\n",
    "            time.sleep(20)\n",
    "        elif index%50==0:\n",
    "            time.sleep(15)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the last index listed above (minus 1) into the index stopped below, repeat until scraping is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_stopped= 1656\n",
    "next_urls= urls[index_stopped:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter= len(next_urls)\n",
    "for index, url in enumerate(next_urls):\n",
    "    df1=Past_Sales(url)\n",
    "    df=pd.concat([df,df1])\n",
    "    counter-=1\n",
    "    print(url, 'Shoes Left:',counter, 'Shoes Done:', len(np.unique(list(df['Shoe'].values))))\n",
    "    if index%10==0:\n",
    "        time.sleep(10)\n",
    "    else:\n",
    "        pass\n",
    "    if index%100==0:\n",
    "        time.sleep(20)\n",
    "    elif index%50==0:\n",
    "        time.sleep(15)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save DataFrames to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nike_df_2019= df\n",
    "#nike_df_2019.to_csv('Nike_Sales_2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reopen all CSV's and put the dataframes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nike_sales = pd.read_csv('Nike_Sales_2019.csv',index_col=0)\n",
    "df_adidas_sales= pd.read_csv('Adidas_Sales_2019.csv',index_col=0)\n",
    "df_jordan_sales = pd.read_csv('Jordan_Sales_2019.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales= pd.concat([df_nike_sales, df_jordan_sales, df_adidas_sales])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoe_names= Shoe_Names(df_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same concept as the for loop above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Shoe_Data_Scraper(shoe_names[0])\n",
    "counter= len(shoe_names)\n",
    "for index, shoe in enumerate(shoe_names):\n",
    "    if shoe==shoe_names[0]:\n",
    "        pass\n",
    "    else:\n",
    "        df1=Shoe_Data_Scraper(shoe)\n",
    "        df=pd.concat([df,df1])\n",
    "        counter-=1\n",
    "        print(shoe, 'Shoes Left:',counter, 'Shoes Done:', len(np.unique(list(df['name'].values))))\n",
    "        if index%10==0:\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            pass\n",
    "        if index%100==0:\n",
    "            time.sleep(20)\n",
    "        elif index%50==0:\n",
    "            time.sleep(15)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shoe_Data_Scraper(shoe_names[125])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the last for loop you will enter the index stopped but here you will put the length of the dataframe and subtract it from the index stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_stopped= 3028-716\n",
    "next_shoes= shoe_names[index_stopped:]\n",
    "\n",
    "counter= len(next_shoes)\n",
    "for index, shoe in enumerate(next_shoes):\n",
    "    if shoe==shoe_names[0]:\n",
    "        pass\n",
    "    else:\n",
    "        df1=Shoe_Data_Scraper(shoe)\n",
    "        df=pd.concat([df,df1])\n",
    "        counter-=1\n",
    "        print(shoe, 'Shoes Left:',counter, 'Shoes Done:', len(np.unique(list(df['name'].values))))\n",
    "        if index%10==0:\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            pass\n",
    "        if index%100==0:\n",
    "            time.sleep(20)\n",
    "        elif index%50==0:\n",
    "            time.sleep(15)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('Shoe_Data_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('Shoe_Data_2019.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove useless columns, drop NaN values and reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['id', 'uuid','condition', 'countryOfManufacture','gender','minimumBid','meta', 'PortfolioItems', 'shipping', 'enhancedImage', 'media','type', 'aLim','shippingGroup', 'traits','charityCondition', 'breadcrumbs', 'market', 'children', 'quantity','sizeLocale', 'sizeTitle', 'sizeDescriptor', 'sizeAllDescriptor', 'description', 'lithiumIonBattery','shortDescription', 'styleId','tickerSymbol','dataType','usHtsCode'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['retailPrice'].dropna(inplace=True)\n",
    "df.reset_index(drop= True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new data frame that has the average shoe sales price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales_df= pd.DataFrame(df_sales.groupby([\"Shoe\"])[\"Price\"].mean())\n",
    "most_common_size=df_sales.groupby(['Shoe']).agg(lambda x:x.value_counts().index[0])\n",
    "most_common_size.drop(columns=['Price', 'Sale_Date'], inplace= True)\n",
    "avg_sales_df= pd.merge(how='right', left=most_common_size, right= avg_sales_df, left_on='Shoe', right_on= 'Shoe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine our two seperate dataframes of information and create new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title']=df['title'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data= pd.merge(how='left', left=df, right= avg_sales_df, left_on='title', right_on= 'Shoe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.rename(columns={\"Price\": \"Avg_Sale\",'Size': \"Best_Size\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['Avg_Return']= combined_data['Avg_Sale']*.875 #gives us our average return after sales fee's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['Avg_Profit']= combined_data['Avg_Return']-combined_data['retailPrice'] #Gives average profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.sort_values(by='Avg_Profit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make releaseDate into a datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['releaseDate']= pd.to_datetime(combined_data['releaseDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.reset_index(drop= True ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combined_data.to_csv('Full_Shoe_Data_2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times when a shoe comes out the release is hyped up by SneakerHead's, lets create a function that can see if people are talking about the sneaker on it's release day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scrape twitter using the Twint API and create a function that can tell if a release is hyped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['twitter']= 'G'\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = twint.Config()\n",
    "#c.Username = \"kicksonfire\"\n",
    "c.Search = \"Nike Air Max 1 Golf\"\n",
    "c.Since = '2019-04-05'\n",
    "c.Until = '2019-04-05'\n",
    "# Run\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def hyped_release(DataFrame,index):\n",
    "    time.sleep(3)\n",
    "    c = twint.Config()\n",
    "    c.Search = DataFrame['shoe'][index]\n",
    "    c.Since = str(DataFrame['releaseDate'][index])\n",
    "    c.Until = str(DataFrame['releaseDate'][index] + datetime.timedelta(days=1))\n",
    "    c.Store_object=True\n",
    "    c.Pandas=True\n",
    "    twint.run.Search(c)\n",
    "    df1= twint.storage.panda.Tweets_df\n",
    "    if len(df1)>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista= []\n",
    "for index in range(len(combined_data)):\n",
    "    lista.append(hyped_release(combined_data,index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['twitter']= lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save our complete dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_data.to_csv('Full_Shoe_Data_2019.csv')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
